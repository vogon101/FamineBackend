{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Required CSVS:\n",
    "\n",
    "data/conflict.csv - raw conflict data\n",
    "\n",
    "data/climate-complete-noaa.csv - raw climate data\n",
    "\n",
    "data/clean_fews.csv - transformed food data from FEWSNET\n",
    "\n",
    "data/clean_food.csv - transformed food data from WFP\n",
    "\n",
    "data/clean_ipc.csv - transformed IPC data from FSNAU\n",
    "\n",
    "\n",
    "For the last 3 csvs, see other notebook on transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pystan\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "There are three response vectors, $A_i$,$B_i$, $C_i$, where $A$ is the % of population in IPC Phase 2 and $B$ is the % of population in IPC Phase 3 and $C_i$ is the % of the population in IPC Phase 4. We have $N$ points of data, and $K$ features to estimate the coefficients of.\n",
    "\n",
    "$$\\mu_{A_i} = \\text{inv_logit}(\\alpha_{A} + \\sum_{X\\in\\{A,B,C\\}}(\\beta_{A,X} * \\text{logit}(X_{i-1})) + \\sum_k(\\text{coeffs}_{A,k} * \\text{feats}_{i,k}))$$\n",
    "\n",
    "$$A_i \\sim \\text{Beta_prop}(\\mu_{A_i}, \\kappa_A)$$\n",
    "\n",
    "$$\\mu_{B_i} = \\text{inv_logit}(\\alpha_{B} + \\sum_{X\\in\\{A,B,C\\}}(\\beta_{B,X} * \\text{logit}(X_{i-1})) + \\sum_k(\\text{coeffs}_{B,k} * \\text{feats}_{i,k}))$$\n",
    "\n",
    "$$B_i \\sim \\text{Beta_prop}(\\mu_{B_i}, \\kappa_B)$$\n",
    "\n",
    "$$\\mu_{C_i} = \\text{inv_logit}(\\alpha_{C} + \\sum_{X\\in\\{A,B,C\\}}(\\beta_{C,X} * \\text{logit}(X_{i-1})) + \\sum_k(\\text{coeffs}_{C,k} * \\text{feats}_{i,k}))$$\n",
    "\n",
    "$$C_i \\sim \\text{Beta_prop}(\\mu_{C_i}, \\kappa_C)$$\n",
    "\n",
    "Beta_prop is a variant of the Beta distribution, with parameters $\\mu$, which is the mean, and $\\kappa$, which is the precision, or the inverse of the variance, where a high $\\kappa$ implies a low variance.\n",
    "\n",
    "To transform $\\mu$ and $\\kappa$ back into the standard parameters $\\alpha$ and $\\beta$:\n",
    "$$ \\alpha = \\mu\\kappa $$\n",
    "$$ \\beta = (1-\\mu)\\kappa $$\n",
    "\n",
    "logit is the logit function $\\text{logit(p)} = \\log(\\frac{p}{1-p})$, and maps $(0,1)$ to $(-\\infty, +\\infty)$, \n",
    "and inv_logit is the inverse function $\\text{inv_logit(x)} = \\frac{e^x}{e^x + 1}$\n",
    "\n",
    "All the parameters and data ending with _2 refers to IPC Phase 2, _3 to IPC Phase 3, and _4 to IPC Phase 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL beta_model_c1f83952f869caec67ffdfc8087592cc NOW.\n"
     ]
    }
   ],
   "source": [
    "beta_model_code = '''\n",
    "data {\n",
    "    int<lower=0> N;\n",
    "    int<lower=0> K;\n",
    "    matrix[N,K] feats;\n",
    "    vector[N] response_2;\n",
    "    vector[N] response_3;\n",
    "    vector[N] response_4;\n",
    "}\n",
    "transformed data {\n",
    "    vector[N] logit_response_2;\n",
    "    vector[N] logit_response_3;\n",
    "    vector[N] logit_response_4;\n",
    "    \n",
    "    logit_response_2 = logit(response_2);\n",
    "    logit_response_3 = logit(response_3);\n",
    "    logit_response_4 = logit(response_4);\n",
    "}\n",
    "parameters{\n",
    "    real alpha_2;\n",
    "    vector[3] beta_2;\n",
    "    vector[K] coeffs_2;\n",
    "    real<lower = 100> kappa_2;\n",
    "    \n",
    "    real alpha_3;\n",
    "    vector[3] beta_3;\n",
    "    vector[K] coeffs_3;\n",
    "    real<lower = 100> kappa_3;\n",
    "    \n",
    "    real alpha_4;\n",
    "    vector[3] beta_4;\n",
    "    vector[K] coeffs_4;\n",
    "    real<lower = 100> kappa_4;\n",
    "}\n",
    "\n",
    "model{\n",
    "    vector[N] mus_2;\n",
    "    vector[N] mus_3;\n",
    "    vector[N] mus_4;\n",
    "    \n",
    "    mus_2[2:N] = inv_logit(alpha_2 + beta_2[1]*logit_response_2[1:(N-1)] + beta_2[2]*logit_response_3[1:(N-1)] + beta_2[3]*logit_response_4[1:(N-1)] + feats[2:N]*coeffs_2);\n",
    "    response_2[2:N] ~ beta_proportion(mus_2[2:N], kappa_2);\n",
    "    \n",
    "    mus_3[2:N] = inv_logit(alpha_3 + beta_3[1]*logit_response_2[1:(N-1)] + beta_3[2]*logit_response_3[1:(N-1)] + beta_3[3]*logit_response_4[1:(N-1)] + feats[2:N]*coeffs_3);\n",
    "    response_3[2:N] ~ beta_proportion(mus_3[2:N], kappa_3);\n",
    "    \n",
    "    mus_4[2:N] = inv_logit(alpha_4 + beta_4[1]*logit_response_2[1:(N-1)] + beta_4[2]*logit_response_3[1:(N-1)] + beta_4[3]*logit_response_4[1:(N-1)] + feats[2:N]*coeffs_4);\n",
    "    response_4[2:N] ~ beta_proportion(mus_4[2:N], kappa_4);\n",
    "}\n",
    "'''\n",
    "beta_model = pystan.StanModel(model_code=beta_model_code, model_name=\"beta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Export models\n",
    "REGIONS = ['Awdal', 'Bakool', 'Banadir', 'Bari', 'Bay', 'Galgaduud', 'Gedo', 'Hiraan', 'Lower Juba', 'Lower Shabelle', 'Middle Juba', 'Middle Shabelle', 'Mudug', 'Nugaal', 'Sanaag', 'Sool', 'Togdheer', 'Woqooyi Galbeed']\n",
    "if (False):\n",
    "    for region in REGIONS[9:]:\n",
    "        datasets = extract_features(region)\n",
    "        if(datasets == None):\n",
    "            print(\"{} has too little data to fit model\".format(region))\n",
    "        else:\n",
    "            print(\"{}\".format(region))\n",
    "            model = fit_model(datasets, 0)\n",
    "            plot_model(datasets, model)\n",
    "            model.to_dataframe().to_csv(\"model/{}.csv\".format(region), index=False)\n",
    "\n",
    "#Saves feature names\n",
    "rows = []\n",
    "\n",
    "for region in REGIONS:\n",
    "    datasets = extract_features(region)\n",
    "    if (datasets != None):\n",
    "        rows.append(dict(region=region, feature_names=list(datasets.values())[0]['feature_names']))\n",
    "feature_names_df = pd.DataFrame(rows)\n",
    "feature_names_df.to_csv(\"model/feature-names.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "MONTHS = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "LEAPS  = [0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "\n",
    "def getDays(year, quarter):\n",
    "    if(year % 4 ==0):\n",
    "        return [_, 91, 91, 92, 92][quarter]\n",
    "    else:\n",
    "        return [_, 90, 91, 92, 92][quarter]\n",
    "\n",
    "def getDate(year, month, day):\n",
    "    if(year % 4 == 0):\n",
    "        return year + (np.cumsum(LEAPS)[month-1]+day-1)/366.\n",
    "    else:\n",
    "        return year + (np.cumsum(MONTHS)[month-1]+day-1)/365."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "#data is per-region\n",
    "def predict_data(data, end_year, end_quarter):\n",
    "    new_data = dict()\n",
    "    feature_names = data['feature_names'][:]\n",
    "    \n",
    "    \n",
    "    food_df = data['food_df'].copy()\n",
    "    ffood_df = data['ffood_df'].copy()\n",
    "    weather_df = data['weather_df'].copy()\n",
    "    conflict_df = data['conflict_df'].copy()\n",
    "    new_data['ipc_df'] = data['ipc_df'].copy()\n",
    "    end_month = [_, 3, 6, 9, 12][end_quarter]\n",
    "    end_day = [_, 31, 30, 30, 31][end_quarter]\n",
    "    \n",
    "    new_rows = []\n",
    "    food_items = sorted(set(food_df.Item.values))\n",
    "    for item in food_items:\n",
    "        \n",
    "        t_item_df = food_df[food_df.Item.eq(item)]\n",
    "        \n",
    "        market = t_item_df.Market.values[0]\n",
    "        region = t_item_df.Region.values[0]\n",
    "        last_year = max(t_item_df.Year.values)\n",
    "        last_quarter = max(t_item_df[t_item_df.Year.eq(last_year)].Quarter.values)\n",
    "        last_month = max(t_item_df[t_item_df.Year.eq(last_year)].Month.values)\n",
    "        last_price = t_item_df.loc[t_item_df.Year.eq(last_year) & t_item_df.Month.eq(last_month)].Price.values[0]\n",
    "        \n",
    "        for new_year in range(last_year, end_year+1):\n",
    "            month_start = last_month+1 if new_year == last_year else 1\n",
    "            month_end = end_month+1 if new_year == end_year else 13\n",
    "            for new_month in range(month_start, month_end):\n",
    "                new_rows.append(dict(\n",
    "                    Date = getDate(new_year, new_month, 1),\n",
    "                    Region = region,\n",
    "                    Market = market,\n",
    "                    Item = item,\n",
    "                    Price = last_price,\n",
    "                    Year = new_year,\n",
    "                    Month = new_month,\n",
    "                    Quarter = math.ceil(new_month/3)\n",
    "                ))\n",
    "    food_df = food_df.append(new_rows).sort_values(by=['Item', 'Date']).reset_index(drop=True)\n",
    "    new_data['food_df'] = food_df\n",
    "    \n",
    "    ffood_items = sorted(set(ffood_df.Item))\n",
    "    new_rows=[]\n",
    "    for item in ffood_items:\n",
    "        \n",
    "        t_item_df = ffood_df[ffood_df.Item.eq(item)]\n",
    "        \n",
    "        market = t_item_df.Market.values[0]\n",
    "        region = t_item_df.Region.values[0]\n",
    "        last_year = max(t_item_df.Year.values)\n",
    "        last_quarter = max(t_item_df[t_item_df.Year.eq(last_year)].Quarter.values)\n",
    "        last_month = max(t_item_df[t_item_df.Year.eq(last_year)].Month.values)\n",
    "        last_price = t_item_df.loc[t_item_df.Year.eq(last_year) & t_item_df.Month.eq(last_month)].Price.values[0]\n",
    "        \n",
    "        for new_year in range(last_year, end_year+1):\n",
    "            if(last_month==12 and new_year == last_year):\n",
    "                continue\n",
    "            month_start = last_month+1 if new_year == last_year else 1\n",
    "            month_end = end_month+1 if new_year == end_year else 13\n",
    "            for new_month in range(month_start, month_end):\n",
    "                new_rows.append(dict(\n",
    "                    Date = getDate(new_year, new_month, 1),\n",
    "                    Region = region,\n",
    "                    Market = market,\n",
    "                    Item = item,\n",
    "                    Price = last_price,\n",
    "                    Year = new_year,\n",
    "                    Month = new_month,\n",
    "                    Quarter = math.ceil(new_month/3)\n",
    "                ))\n",
    "    ffood_df = ffood_df.append(new_rows).sort_values(by=['Item', 'Date']).reset_index(drop=True)\n",
    "    new_data['ffood_df'] = ffood_df\n",
    "    \n",
    "    new_rows = []\n",
    "    region = conflict_df.Region.values[0]\n",
    "    last_year = max(conflict_df.Year.values)\n",
    "    last_month = max(conflict_df[conflict_df.Year.eq(last_year)].Month.values)\n",
    "    last_day = max(conflict_df[conflict_df.Year.eq(last_year) & conflict_df.Month.eq(last_month)].Day.values)\n",
    "    last_fatality = conflict_df.loc[conflict_df.Year.eq(last_year) & conflict_df.Month.eq(last_month) & conflict_df.Day.eq(last_day)].Fatalities.values[0]\n",
    "    \n",
    "    for new_year in range(last_year, end_year+1):\n",
    "        month_start = last_month+1 if new_year == last_year else 1\n",
    "        month_end = end_month+1 if new_year == end_year else 13\n",
    "        for new_month in range(month_start, month_end):\n",
    "            day_start = last_day+1 if (new_year == last_year and new_month == last_month) else 1\n",
    "            day_end = end_day+1 if (new_year == end_year and new_month == end_month) else MONTHS[new_month]+1\n",
    "            if(new_month == 2 and new_year%4 == 0):\n",
    "                day_end+=1\n",
    "            for new_day in range(day_start, day_end):\n",
    "                new_rows.append(dict(\n",
    "                    Region = region,\n",
    "                    Date = getDate(new_year, new_month, new_day),\n",
    "                    Fatalities = last_fatality,\n",
    "                    Year = new_year,\n",
    "                    Month = new_month,\n",
    "                    Day = new_day,\n",
    "                    Quarter = math.ceil(new_month/3)\n",
    "                ))\n",
    "    conflict_df = conflict_df.append(new_rows).sort_values(by=['Date']).reset_index(drop=True)\n",
    "    new_data['conflict_df'] = conflict_df\n",
    "    \n",
    "    new_rows = []\n",
    "    station = weather_df.Station.values[0]\n",
    "    last_year = max(weather_df.Year.values)\n",
    "    last_month = max(weather_df[weather_df.Year.eq(last_year)].Month.values)\n",
    "    last_day = max(weather_df[weather_df.Year.eq(last_year) & weather_df.Month.eq(last_month)].Day.values)\n",
    "    last_temperature = weather_df[weather_df.Year.eq(last_year) & weather_df.Month.eq(last_month) & weather_df.Day.eq(last_day)].Temperature.values[0]\n",
    "    \n",
    "    for new_year in range(last_year, end_year+1):\n",
    "        month_start = last_month+1 if new_year == last_year else 1\n",
    "        month_end = end_month+1 if new_year == end_year else 13\n",
    "        for new_month in range(month_start, month_end):\n",
    "            day_start = last_day+1 if (new_year == last_year and new_month == last_month) else 1\n",
    "            day_end = end_day+1 if (new_year == end_year and new_month == end_month) else MONTHS[new_month]+1\n",
    "            if(new_month == 2 and new_year%4 == 0):\n",
    "                day_end+=1\n",
    "            for new_day in range(day_start, day_end):\n",
    "                new_rows.append(dict(\n",
    "                    Station = station,\n",
    "                    Date = getDate(new_year, new_month, new_day),\n",
    "                    Temperature = last_temperature,\n",
    "                    Year = new_year,\n",
    "                    Month = new_month,\n",
    "                    Day = new_day,\n",
    "                    Quarter = math.ceil(new_month/3)\n",
    "                ))\n",
    "                \n",
    "    weather_df = weather_df.append(new_rows).sort_values(by=['Date']).reset_index(drop=True)\n",
    "    new_data['weather_df'] = weather_df\n",
    "    \n",
    "    datasets = copy.deepcopy(data['datasets'])\n",
    "    \n",
    "    last_date = max(datasets.keys())\n",
    "    last_year = last_date//10\n",
    "    last_quarter = last_date%10\n",
    "    \n",
    "    for new_year in range(last_year, end_year+1):\n",
    "        quarter_start = last_quarter+1 if new_year == last_year else 1\n",
    "        quarter_end = end_quarter+1 if new_year == end_year else 5\n",
    "        for new_quarter in range(quarter_start, quarter_end):\n",
    "            nDays = getDays(new_year, new_quarter)\n",
    "            dataset = dict()\n",
    "            features=  []\n",
    "            \n",
    "            for item in food_items:\n",
    "                market = food_df[food_df.Item.eq(item)].Market.values[0]\n",
    "                t_item_df = food_df[food_df.Year.eq(new_year) & food_df.Quarter.eq(new_quarter) & food_df.Item.eq(item)]\n",
    "                features.append(np.mean(t_item_df.Price.values)/1e4)\n",
    "                \n",
    "            for item in ffood_items:\n",
    "                market = ffood_df[ffood_df.Item.eq(item)].Market.values[0]\n",
    "                t_item_df = ffood_df[ffood_df.Year.eq(new_year) & ffood_df.Quarter.eq(new_quarter) & ffood_df.Item.eq(item)]\n",
    "                features.append(np.mean(t_item_df.Price.values)/1e4)\n",
    "\n",
    "            t_conflict_df = conflict_df[conflict_df.Year.eq(new_year) & conflict_df.Quarter.eq(new_quarter)]\n",
    "            features.append(np.sum(t_conflict_df.Fatalities.values)/nDays)\n",
    "\n",
    "            t_weather_df = weather_df[weather_df.Year.eq(new_year) & weather_df.Quarter.eq(new_quarter)]\n",
    "            features.append(np.mean(t_weather_df.Temperature.values))\n",
    "\n",
    "            dataset['features'] = features\n",
    "            datasets[new_year*10+new_quarter] = dataset\n",
    "    new_data['datasets'] = datasets\n",
    "    return new_data\n",
    "    \n",
    "    \n",
    "bakool_data = extract_features(\"Banadir\")\n",
    "predict_data(bakool_data, 2020, 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "extract_features(region):\n",
    "\n",
    "returns datasets, a dictionary where the keys are dates and the values are dictionaries.\n",
    "\n",
    "Each dictionary has the following keys and values:\n",
    "'features' - list of feature values\n",
    "'feature_names' - name of each feature\n",
    "'food' - a Pandas dataframe of food information\n",
    "'conflict' - a Pandas dataframe of conflict information\n",
    "'weather' - a Pandas dataframe of weather information\n",
    "'ipc' - a dictionary:\n",
    "    'p2perc': Proportion in IPC Phase 2\n",
    "    'p3perc': Proportion in IPC Phase 3\n",
    "    'p4perc': Proportion in IPC Phase 4\n",
    "    \n",
    "    \n",
    "Some regions have insufficient data, they will return None if extract_features is called\n",
    "'''\n",
    "\n",
    "def extract_features(region):\n",
    "    #CONSTANTS\n",
    "    WANTEDFEWSFOOD = {'Cowpeas (Red)'}\n",
    "    WANTEDSTATIONS = {'EGAL INTL'}\n",
    "    \n",
    "    data = dict()\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    food_df = pd.read_csv('data/clean_food.csv')\n",
    "    food_df = food_df[food_df.Region.eq(region)]\n",
    "    data['food_df'] = food_df\n",
    "    \n",
    "    food_items = sorted(set(food_df.Item.values))\n",
    "    for food_item in food_items:\n",
    "        market = food_df[food_df.Item.eq(food_item)].Market.values[0]\n",
    "        feature_names.append(\"{} - {}\".format(food_item, market))\n",
    "    \n",
    "\n",
    "    #Constrained by food data dates, get the earliest and latest dates here:\n",
    "    e_y = min(food_df.Year.values)\n",
    "    e_q = min(food_df[food_df.Year.eq(e_y)].Quarter.values)\n",
    "    l_y = max(food_df.Year.values)\n",
    "    l_q = max(food_df[food_df.Year.eq(l_y)].Quarter.values)\n",
    "    \n",
    "    ffood_df = pd.read_csv('data/clean_fews.csv')\n",
    "    ffood_df = ffood_df[ffood_df.Item.isin(WANTEDFEWSFOOD) & ffood_df.Region.eq(region)]\n",
    "    ffood_df = ffood_df[(ffood_df.Year.eq(e_y) & ffood_df.Quarter.ge(e_q)) | (ffood_df.Year.gt(e_y))]\n",
    "    ffood_df = ffood_df[(ffood_df.Year.eq(l_y) & ffood_df.Quarter.le(l_q)) | (ffood_df.Year.lt(l_y))]\n",
    "    data['ffood_df'] = ffood_df\n",
    "    \n",
    "    ffood_items = sorted(set(ffood_df.Item.values))\n",
    "    for ffood_item in ffood_items:\n",
    "        market = ffood_df[ffood_df.Item.eq(ffood_item)].Market.values[0]\n",
    "        feature_names.append(\"{} - {}\".format(ffood_item, market))\n",
    "\n",
    "    \n",
    "    conflict_df = pd.read_csv(\"data/clean_conflict.csv\")\n",
    "    conflict_df = conflict_df[conflict_df.Region.eq(region)]\n",
    "    conflict_df = conflict_df[(conflict_df.Year.eq(e_y) & conflict_df.Quarter.ge(e_q)) | (conflict_df.Year.gt(e_y))]\n",
    "    conflict_df = conflict_df[(conflict_df.Year.eq(l_y) & conflict_df.Quarter.le(l_q)) | (conflict_df.Year.lt(l_y))]\n",
    "    data['conflict_df'] = conflict_df\n",
    "    \n",
    "    feature_names.append(\"Fatalities\")\n",
    "\n",
    "    \n",
    "    ipc_df = pd.read_csv('data/clean_ipc.csv')\n",
    "    ipc_df = ipc_df[ipc_df.Region.eq(region)]\n",
    "    ipc_df = ipc_df[(ipc_df.Year.eq(e_y) & ipc_df.Quarter.ge(e_q)) | (ipc_df.Year.gt(e_y))]\n",
    "    ipc_df = ipc_df[(ipc_df.Year.eq(l_y) & ipc_df.Quarter.le(l_q)) | (ipc_df.Year.lt(l_y))]\n",
    "    data['ipc_df'] = ipc_df\n",
    "    \n",
    "\n",
    "\n",
    "    #Extract weather data\n",
    "    weather_df = pd.read_csv('data/clean_weather.csv')\n",
    "    weather_df = weather_df[weather_df.Station.isin(WANTEDSTATIONS)]\n",
    "    weather_df = weather_df[(weather_df.Year.eq(e_y) & weather_df.Quarter.ge(e_q)) | (weather_df.Year.gt(e_y))]\n",
    "    weather_df = weather_df[(weather_df.Year.eq(l_y) & weather_df.Quarter.le(l_q)) | (weather_df.Year.lt(l_y))]\n",
    "    data['weather_df'] = weather_df\n",
    "    \n",
    "    feature_names.append(\"Temperature\")\n",
    "\n",
    "    if(len(ipc_df)<=12):\n",
    "        #Insufficient data, trying to fit with data will give an over-fitted model\n",
    "        return None\n",
    "    \n",
    "    datasets = dict()\n",
    "    data['feature_names'] = feature_names\n",
    "    \n",
    "    \n",
    "    def getDays(year, quarter):\n",
    "        if(year % 4 ==0):\n",
    "            return [_, 91, 91, 92, 92][quarter]\n",
    "        else:\n",
    "            return [_, 90, 91, 92, 92][quarter]\n",
    "    \n",
    "    \n",
    "    for (i,row) in ipc_df.iterrows():\n",
    "        year = row.Year\n",
    "        quarter = row.Quarter\n",
    "        nDays = getDays(year, quarter)\n",
    "        dataset = dict()\n",
    "        \n",
    "        dataset['P2'] =  row.P2perc\n",
    "        dataset['P3'] =  row.P3perc\n",
    "        dataset['P4'] =  row.P4perc\n",
    "\n",
    "        features = []\n",
    "        \n",
    "        \n",
    "        for item in food_items:\n",
    "            market = food_df[food_df.Item.eq(item)].Market.values[0]\n",
    "            t_item_df = food_df[food_df.Year.eq(year) & food_df.Quarter.eq(quarter) & food_df.Item.eq(item)]\n",
    "            #dataset[\"{} - {}\".format(item, market)] = t_item_df\n",
    "            features.append(np.mean(t_item_df.Price.values)/1e4)\n",
    "            \n",
    "        \n",
    "        for item in ffood_items:\n",
    "            market = ffood_df[ffood_df.Item.eq(item)].Market.values[0]\n",
    "            t_item_df = ffood_df[ffood_df.Year.eq(year) & ffood_df.Quarter.eq(quarter) & ffood_df.Item.eq(item)]\n",
    "            #dataset[\"{} - {}\".format(item, market)] = t_item_df\n",
    "            features.append(np.mean(t_item_df.Price.values)/1e4)\n",
    "        \n",
    "        t_conflict_df = conflict_df[conflict_df.Year.eq(year) & conflict_df.Quarter.eq(quarter)]\n",
    "        #dataset[\"Fatalities\"] = t_conflict_df\n",
    "        features.append(np.sum(t_conflict_df.Fatalities.values)/nDays)\n",
    "        \n",
    "        t_weather_df = weather_df[weather_df.Year.eq(year) & weather_df.Quarter.eq(quarter)]\n",
    "        cycle = 1\n",
    "        \n",
    "        while(len(t_weather_df) < 10):\n",
    "            t_weather_df = weather_df[weather_df.Year.eq(year+cycle) & weather_df.Quarter.eq(quarter)]\n",
    "            cycle += 1\n",
    "        #dataset[\"Temperature\"] = t_weather_df\n",
    "        features.append(np.mean(t_weather_df.Temperature.values))\n",
    "        \n",
    "        dataset['features'] = features\n",
    "        datasets[year*10+quarter] = dataset\n",
    "    data['datasets'] = datasets\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fit_model(datasets, holdout)\n",
    "Attempts to fit datasets, but ignoring out the last <holdout> dates for evaluation\n",
    "Returns a StanFit4Model object\n",
    "'''\n",
    "def fit_model(datasets, holdout):\n",
    "    train_dates = sorted(list (datasets.keys()))[:-holdout] if (holdout>0) else sorted(list (datasets.keys()))\n",
    "    nFeatures = len(datasets[train_dates[0]]['features'])\n",
    "    features = [datasets[date]['features'] for date in train_dates]\n",
    "    response_2 = [max(datasets[date]['P2'], 1e-5) for date in train_dates]\n",
    "    response_3 = [max(datasets[date]['P3'], 1e-5)  for date in train_dates]\n",
    "    response_4 = [max(datasets[date]['P4'], 1e-5)  for date in train_dates]\n",
    "    famine_model_data = dict(\n",
    "        N = len(train_dates),\n",
    "        K = nFeatures,\n",
    "        feats = features,\n",
    "        response_2 = response_2,\n",
    "        response_3 = response_3,\n",
    "        response_4 = response_4\n",
    "    )\n",
    "    result = beta_model.sampling(data=famine_model_data, iter=3000, control = dict(max_treedepth=12, adapt_delta=0.8))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#bakool_model = fit_model(bakool_data['datasets'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:15 of 6000 iterations ended with a divergence (0.25 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "WARNING:pystan:5942 of 6000 iterations saturated the maximum tree depth of 12 (99 %)\n",
      "WARNING:pystan:Run again with max_treedepth larger than 12 to avoid saturation\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c05b111b289e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbakool_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bakool\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbakool_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbakool_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datasets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbakool_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datasets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbakool_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_model' is not defined"
     ]
    }
   ],
   "source": [
    "bakool_data = extract_features(\"Bakool\")\n",
    "bakool_model = fit_model(bakool_data['datasets'],3)\n",
    "plot_model(bakool_data['datasets'], bakool_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot_model(datasets, model)\n",
    "Plots the predicted and actual IPC Phase proportion values based on the datasets and the StanFit4Model object\n",
    "'''\n",
    "\n",
    "def plot_model(datasets, model):\n",
    "    def getDate(date):\n",
    "        year = date//10\n",
    "        quarter = date%10\n",
    "        if(year % 4 ==0):\n",
    "            return year+(np.cumsum([0, 91, 91, 92, 92])[quarter]/366.)\n",
    "        else:\n",
    "            return year+(np.cumsum([0, 90, 91, 92, 92])[quarter]/365.)\n",
    "    \n",
    "    \n",
    "    nFeatures = len(datasets[list(datasets.keys())[0]]['features'])\n",
    "    all_coeffs = list(map(lambda x: sum(x)/len(x), model.get_posterior_mean()))\n",
    "    al_2, be_2, co_2, k_2 = all_coeffs[0],all_coeffs[1:4],all_coeffs[4:4+nFeatures],all_coeffs[4+nFeatures]\n",
    "    al_3, be_3, co_3, k_3 = all_coeffs[5+nFeatures],all_coeffs[6+nFeatures:9+nFeatures],all_coeffs[9+nFeatures:9+2*nFeatures],all_coeffs[9+2*nFeatures]\n",
    "    al_4, be_4, co_4, k_4 = all_coeffs[10+2*nFeatures],all_coeffs[11+2*nFeatures:14+2*nFeatures],all_coeffs[14+2*nFeatures:14+3*nFeatures],all_coeffs[14+3*nFeatures]\n",
    "    \n",
    "    dates = sorted(list(datasets.keys()))\n",
    "    \n",
    "    gold_ipc2 = [max(datasets[date]['P2'], 1e-5) for date in dates]\n",
    "    gold_ipc3 = [max(datasets[date]['P3'], 1e-5) for date in dates]\n",
    "    gold_ipc4 = [max(datasets[date]['P4'], 1e-5) for date in dates]\n",
    "    \n",
    "    gold_ipc2_logit = list(map(scipy.special.logit, gold_ipc2))\n",
    "    gold_ipc3_logit = list(map(scipy.special.logit, gold_ipc3))\n",
    "    gold_ipc4_logit = list(map(scipy.special.logit, gold_ipc4))\n",
    "    \n",
    "    \n",
    "    pred_ipc2_logit = [gold_ipc2_logit[0]]\n",
    "    pred_ipc3_logit = [gold_ipc3_logit[0]]\n",
    "    pred_ipc4_logit = [gold_ipc3_logit[0]]\n",
    "    \n",
    "    \n",
    "    for (i, time) in enumerate(dates[1:]):\n",
    "        new_ipc_2a = al_2 + be_2[0]*gold_ipc2_logit[i] + be_2[1]*gold_ipc3_logit[i] + be_2[2]*gold_ipc4_logit[i] + sum(np.multiply(co_2, datasets[time]['features']))\n",
    "        new_ipc_3a = al_3 + be_3[0]*gold_ipc2_logit[i] + be_3[1]*gold_ipc3_logit[i] + be_3[2]*gold_ipc4_logit[i] + sum(np.multiply(co_3, datasets[time]['features']))\n",
    "        new_ipc_4a = al_4 + be_4[0]*gold_ipc2_logit[i] + be_4[1]*gold_ipc3_logit[i] + be_4[2]*gold_ipc4_logit[i] + sum(np.multiply(co_4, datasets[time]['features']))\n",
    "        \n",
    "        pred_ipc2_logit.append(new_ipc_2a)\n",
    "        pred_ipc3_logit.append(new_ipc_3a)\n",
    "        pred_ipc4_logit.append(new_ipc_4a)\n",
    "        \n",
    "    pred_ipc2 = list(map(scipy.special.expit, pred_ipc2_logit))\n",
    "    pred_ipc3 = list(map(scipy.special.expit, pred_ipc3_logit))\n",
    "    pred_ipc4 = list(map(scipy.special.expit, pred_ipc4_logit))\n",
    "    \n",
    "    \n",
    "    float_dates = list(map(getDate, dates))\n",
    "    print(float_dates)\n",
    "    print(\"IPC PHASE 2\")\n",
    "    plt.plot(float_dates, gold_ipc2, marker='o', color='gold')\n",
    "    plt.plot(float_dates,pred_ipc2, marker='+', color='red')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"IPC PHASE 3\")\n",
    "    plt.plot(float_dates, gold_ipc3, marker='o', color='gold')\n",
    "    plt.plot(float_dates,pred_ipc3, marker='+', color='blue')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"IPC PHASE 4\")\n",
    "    plt.plot(float_dates, gold_ipc4, marker='o', color='gold')\n",
    "    plt.plot(float_dates,pred_ipc4, marker='+', color='brown')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
