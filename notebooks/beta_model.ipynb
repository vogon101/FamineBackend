{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Required CSVS:\n",
    "\n",
    "data/conflict.csv - raw conflict data\n",
    "\n",
    "data/climate-complete-noaa.csv - raw climate data\n",
    "\n",
    "data/clean_fews.csv - transformed food data from FEWSNET\n",
    "\n",
    "data/clean_food.csv - transformed food data from WFP\n",
    "\n",
    "data/clean_ipc.csv - transformed IPC data from FSNAU\n",
    "\n",
    "\n",
    "For the last 3 csvs, see other notebook on transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'. Cambridge\\\\1. Work\\\\3. Courses\\\\1B\\\\Group Project\\\\BackendProto\\\\notebooks'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 13
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pystan\n",
    "import datetime as dt\n",
    "import scipy.special\n",
    "import os\n",
    "os.getcwd()[30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "There are three response vectors, $A_i$,$B_i$, $C_i$, where $A$ is the % of population in IPC Phase 2 and $B$ is the % of population in IPC Phase 3 and $C_i$ is the % of the population in IPC Phase 4. We have $N$ points of data, and $K$ features to estimate the coefficients of.\n",
    "\n",
    "$$\\mu_{A_i} = \\text{inv_logit}(\\alpha_{A} + \\sum_{X\\in\\{A,B,C\\}}(\\beta_{A,X} * \\text{logit}(X_{i-1})) + \\sum_k(\\text{coeffs}_{A,k} * \\text{feats}_{i,k}))$$\n",
    "\n",
    "$$A_i \\sim \\text{Beta_prop}(\\mu_{A_i}, \\kappa_A)$$\n",
    "\n",
    "$$\\mu_{B_i} = \\text{inv_logit}(\\alpha_{B} + \\sum_{X\\in\\{A,B,C\\}}(\\beta_{B,X} * \\text{logit}(X_{i-1})) + \\sum_k(\\text{coeffs}_{B,k} * \\text{feats}_{i,k}))$$\n",
    "\n",
    "$$B_i \\sim \\text{Beta_prop}(\\mu_{B_i}, \\kappa_B)$$\n",
    "\n",
    "$$\\mu_{C_i} = \\text{inv_logit}(\\alpha_{C} + \\sum_{X\\in\\{A,B,C\\}}(\\beta_{C,X} * \\text{logit}(X_{i-1})) + \\sum_k(\\text{coeffs}_{C,k} * \\text{feats}_{i,k}))$$\n",
    "\n",
    "$$C_i \\sim \\text{Beta_prop}(\\mu_{C_i}, \\kappa_C)$$\n",
    "\n",
    "Beta_prop is a variant of the Beta distribution, with parameters $\\mu$, which is the mean, and $\\kappa$, which is the precision, or the inverse of the variance, where a high $\\kappa$ implies a low variance.\n",
    "\n",
    "To transform $\\mu$ and $\\kappa$ back into the standard parameters $\\alpha$ and $\\beta$:\n",
    "$$ \\alpha = \\mu\\kappa $$\n",
    "$$ \\beta = (1-\\mu)\\kappa $$\n",
    "\n",
    "logit is the logit function $\\text{logit(p)} = \\log(\\frac{p}{1-p})$, and maps $(0,1)$ to $(-\\infty, +\\infty)$, \n",
    "and inv_logit is the inverse function $\\text{inv_logit(x)} = \\frac{e^x}{e^x + 1}$\n",
    "\n",
    "All the parameters and data ending with _2 refers to IPC Phase 2, _3 to IPC Phase 3, and _4 to IPC Phase 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL beta_model_c1f83952f869caec67ffdfc8087592cc NOW.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "beta_model_code = '''\n",
    "data {\n",
    "    int<lower=0> N;\n",
    "    int<lower=0> K;\n",
    "    matrix[N,K] feats;\n",
    "    vector[N] response_2;\n",
    "    vector[N] response_3;\n",
    "    vector[N] response_4;\n",
    "}\n",
    "transformed data {\n",
    "    vector[N] logit_response_2;\n",
    "    vector[N] logit_response_3;\n",
    "    vector[N] logit_response_4;\n",
    "    \n",
    "    logit_response_2 = logit(response_2);\n",
    "    logit_response_3 = logit(response_3);\n",
    "    logit_response_4 = logit(response_4);\n",
    "}\n",
    "parameters{\n",
    "    real alpha_2;\n",
    "    vector[3] beta_2;\n",
    "    vector[K] coeffs_2;\n",
    "    real<lower = 100> kappa_2;\n",
    "    \n",
    "    real alpha_3;\n",
    "    vector[3] beta_3;\n",
    "    vector[K] coeffs_3;\n",
    "    real<lower = 100> kappa_3;\n",
    "    \n",
    "    real alpha_4;\n",
    "    vector[3] beta_4;\n",
    "    vector[K] coeffs_4;\n",
    "    real<lower = 100> kappa_4;\n",
    "}\n",
    "\n",
    "model{\n",
    "    vector[N] mus_2;\n",
    "    vector[N] mus_3;\n",
    "    vector[N] mus_4;\n",
    "    \n",
    "    mus_2[2:N] = inv_logit(alpha_2 + beta_2[1]*logit_response_2[1:(N-1)] + beta_2[2]*logit_response_3[1:(N-1)] + beta_2[3]*logit_response_4[1:(N-1)] + feats[2:N]*coeffs_2);\n",
    "    response_2[2:N] ~ beta_proportion(mus_2[2:N], kappa_2);\n",
    "    \n",
    "    mus_3[2:N] = inv_logit(alpha_3 + beta_3[1]*logit_response_2[1:(N-1)] + beta_3[2]*logit_response_3[1:(N-1)] + beta_3[3]*logit_response_4[1:(N-1)] + feats[2:N]*coeffs_3);\n",
    "    response_3[2:N] ~ beta_proportion(mus_3[2:N], kappa_3);\n",
    "    \n",
    "    mus_4[2:N] = inv_logit(alpha_4 + beta_4[1]*logit_response_2[1:(N-1)] + beta_4[2]*logit_response_3[1:(N-1)] + beta_4[3]*logit_response_4[1:(N-1)] + feats[2:N]*coeffs_4);\n",
    "    response_4[2:N] ~ beta_proportion(mus_4[2:N], kappa_4);\n",
    "}\n",
    "'''\n",
    "beta_model = pystan.StanModel(model_code=beta_model_code, model_name=\"beta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7f424a2ede5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mregion\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mREGIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} has too little data to fit model\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-48c2e444d6d1>\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(region)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m      \u001b[1;31m#Transform food data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mfood_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/clean_food.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mfood_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfood_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfood_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mfood_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTARTTIME\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtrimmed_food_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfood_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Item'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Price'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Market'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\scicomp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\scicomp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\scicomp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\scicomp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\scicomp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File data/clean_food.csv does not exist: 'data/clean_food.csv'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/clean_food.csv does not exist: 'data/clean_food.csv'",
     "output_type": "error"
    }
   ],
   "source": [
    "#Export models\n",
    "REGIONS = ['Awdal', 'Bakool', 'Banadir', 'Bari', 'Bay', 'Galgaduud', 'Gedo', 'Hiraan', 'Lower Juba', 'Lower Shabelle', 'Middle Juba', 'Middle Shabelle', 'Mudug', 'Nugaal', 'Sanaag', 'Sool', 'Togdheer', 'Woqooyi Galbeed']\n",
    "if (True):\n",
    "    for region in REGIONS[9:]:\n",
    "        datasets = extract_features(region)\n",
    "        if(datasets == None):\n",
    "            print(\"{} has too little data to fit model\".format(region))\n",
    "        else:\n",
    "            print(\"{}\".format(region))\n",
    "            model = fit_model(datasets, 0)\n",
    "            plot_model(datasets, model)\n",
    "            model.to_dataframe().to_csv(\"model/{}.csv\".format(region), index=False)\n",
    "\n",
    "#Saves feature names\n",
    "rows = []\n",
    "\n",
    "for region in REGIONS:\n",
    "    datasets = extract_features(region)\n",
    "    if (datasets != None):\n",
    "        rows.append(dict(region=region, feature_names=list(datasets.values())[0]['feature_names']))\n",
    "feature_names_df = pd.DataFrame(rows)\n",
    "feature_names_df.to_csv(\"model/feature-names.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Helper Functions for extract_features\n",
    "def get_days(t1, t2):\n",
    "    y1, y2 = int(t1), int(t2)\n",
    "    if(y1 == y2):\n",
    "        return int((t2-t1)*(366 if y2%4==0 else 365))\n",
    "    else:\n",
    "        return (366-274)\n",
    "\n",
    "def get_prev_quarter(t):\n",
    "    year = t//1\n",
    "    days = int(round((t-year) * (366 if year%4==0 else 365)))\n",
    "    if(days == 90 or days == 91):\n",
    "        return year\n",
    "    elif(days == 181):\n",
    "        return year + 90/365\n",
    "    elif(days == 182):\n",
    "        return year + 91/366\n",
    "    elif(days == 273):\n",
    "        return year + 181/365\n",
    "    elif(days == 274):\n",
    "        return year + 182/366\n",
    "    elif(days == 0):\n",
    "        return ((year-1)+274/366) if ((year-1)%4==0) else ((year-1) + 273/365)\n",
    "    else:\n",
    "        print(days, t)\n",
    "\n",
    "\n",
    "def strdate(s):\n",
    "    date = dt.datetime.strptime(s, '%d %B %Y')\n",
    "    year = date.date().year\n",
    "    return year + (date.date().timetuple().tm_yday-1)/ (366 if year%4==0 else 365)\n",
    "MONTHS = list(np.cumsum([0, 31,28,31,30,31,30,31,31,30,31,30,31]))\n",
    "LEAPS = list(np.cumsum([0, 31,29,31,30,31,30,31,31,30,31,30,31]))\n",
    "\n",
    "def dateToFloat(d):\n",
    "    year= d//10000\n",
    "    mo = (d%10000)//100 - 1\n",
    "    day = d%100 - 1\n",
    "    if(year%4==0):\n",
    "        return year+(LEAPS[mo]+day)/366\n",
    "    else:\n",
    "        return year+(MONTHS[mo]+day)/365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "extract_features(region):\n",
    "\n",
    "returns datasets, a dictionary where the keys are dates and the values are dictionaries.\n",
    "\n",
    "Each dictionary has the following keys and values:\n",
    "'features' - list of feature values\n",
    "'feature_names' - name of each feature\n",
    "'food' - a Pandas dataframe of food information\n",
    "'conflict' - a Pandas dataframe of conflict information\n",
    "'weather' - a Pandas dataframe of weather information\n",
    "'ipc' - a dictionary:\n",
    "    'p2perc': Proportion in IPC Phase 2\n",
    "    'p3perc': Proportion in IPC Phase 3\n",
    "    'p4perc': Proportion in IPC Phase 4\n",
    "    \n",
    "    \n",
    "Some regions have insufficient data, they will return None if extract_features is called\n",
    "'''\n",
    "\n",
    "def extract_features(region):\n",
    "    #CONSTANTS\n",
    "    STARTTIME = 2016\n",
    "    WANTEDFEWSFOOD = {'Cowpeas (Red)'}\n",
    "    WANTEDSTATIONS = {'EGAL INTL'}\n",
    "    \n",
    "     #Transform food data:\n",
    "    food_df = pd.read_csv('data/clean_food.csv')\n",
    "    food_df = food_df[food_df.Region.eq(region) & food_df.Date.ge(STARTTIME-1)]\n",
    "    trimmed_food_df = food_df[['Date', 'Item', 'Price', 'Market']]\n",
    "\n",
    "    #Constrained by food data dates, get the earliest and latest dates here:\n",
    "    earliest_date = min(trimmed_food_df.Date.values)\n",
    "    latest_date = max(trimmed_food_df.Date.values)\n",
    "\n",
    "    #Extract FEWSNET food data\n",
    "    ffood_df = pd.read_csv('data/clean_fews.csv')\n",
    "    ffood_df = ffood_df[ffood_df.Date.ge(earliest_date) & ffood_df.Date.le(latest_date) & ffood_df.Item.isin(WANTEDFEWSFOOD) & ffood_df.Region.eq(region)]\n",
    "    ffood_df = ffood_df[['Date', 'Item', 'Price', 'Market']]\n",
    "\n",
    "    #Extract conflict data\n",
    "    conflict_df = pd.read_csv(\"data/conflict.csv\")\n",
    "    conflict_df = conflict_df[['event_date', 'admin1', 'fatalities']].rename(columns={'admin1':'region'})\n",
    "    conflict_df['date'] = conflict_df.event_date.apply(strdate)\n",
    "    trimmed_conflict_df = conflict_df[conflict_df.date.ge(STARTTIME-1) & conflict_df.region.eq(region)].rename(columns={\n",
    "        \"date\": \"Date\",\n",
    "        \"region\": \"Region\",\n",
    "        \"fatalities\": \"Fatalities\"\n",
    "    }).sort_values(by='Date')[['Date','Region','Fatalities']]\n",
    "\n",
    "    #Extract IPC data\n",
    "    ipc_df = pd.read_csv('data/clean-ipc.csv')\n",
    "    trimmed_ipc_df = ipc_df[ipc_df.time.ge(STARTTIME) & ipc_df.region.eq(region)]\n",
    "\n",
    "\n",
    "    #Extract weather data\n",
    "    weather_df = pd.read_csv('data/climate-complete-noaa.csv', usecols = [0,3,4])\n",
    "    weather_df = weather_df[weather_df['Station Name'].isin(WANTEDSTATIONS)]\n",
    "    weather_df['Date'] = weather_df['Date'].apply(dateToFloat)\n",
    "    weather_df = weather_df[weather_df.Date.ge(STARTTIME-1)]\n",
    "    trimmed_weather_df = weather_df.rename(columns={'Station Name':'Station', 'Mean Temperature':'Temperature'})\n",
    "\n",
    "    all_dates = sorted(list(set(trimmed_ipc_df.time.values)))\n",
    "    valid_dates = []\n",
    "    for date in all_dates:\n",
    "        if(date - .125 >= earliest_date and date <= latest_date + .125):\n",
    "            valid_dates.append(date)\n",
    "\n",
    "    if(len(valid_dates)<=12):\n",
    "        #Insufficient data, trying to fit with data will give an over-fitted model\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    datasets = dict()\n",
    "    for date in valid_dates:\n",
    "        prev_q = get_prev_quarter(date)\n",
    "        dataset = dict()\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        dataset['ipc']= trimmed_ipc_df.loc[trimmed_ipc_df.time.eq(date), ['p2perc', 'p3perc', 'p4perc']].to_dict('records')[0]\n",
    "\n",
    "        food_data = dict()\n",
    "        items = sorted(set(trimmed_food_df.Item.values))\n",
    "        for item in items:\n",
    "            food_data[item] = trimmed_food_df[trimmed_food_df.Date.ge(prev_q) & trimmed_food_df.Date.lt(date) & trimmed_food_df.Item.eq(item)]\n",
    "            features.append(np.mean(food_data[item].Price.values)/1e4)\n",
    "            feature_names.append(\"{} - {}\".format(food_data[item].Market.values[0], item))\n",
    "\n",
    "        for food in WANTEDFEWSFOOD:\n",
    "            food_data[food] = ffood_df[ffood_df.Date.ge(prev_q) & ffood_df.Date.lt(date) & ffood_df.Item.eq(food)]\n",
    "            feature = np.mean(food_data[food].Price.values)/1e4\n",
    "            if(not np.isnan(feature)):\n",
    "                features.append(np.mean(food_data[food].Price.values)/1e4)\n",
    "                feature_names.append(\"{} - {}\".format(food_data[item].Market.values[0], item))\n",
    "        \n",
    "        dataset['food'] = food_data\n",
    "\n",
    "        conflict_data = trimmed_conflict_df[trimmed_conflict_df.Date.ge(prev_q)&trimmed_conflict_df.Date.lt(date)]\n",
    "        features.append(np.sum(conflict_data.Fatalities.values)/get_days(prev_q,date))\n",
    "        feature_names.append(\"Fatalities due to Conflict\")\n",
    "        dataset['conflict'] = conflict_data\n",
    "\n",
    "        weather_data = dict()\n",
    "        for station in WANTEDSTATIONS:\n",
    "            weather_data[station] = trimmed_weather_df[trimmed_weather_df.Date.ge(prev_q) & trimmed_weather_df.Date.lt(date) & trimmed_weather_df.Station.eq(station)][['Date','Temperature']]\n",
    "            if(len(weather_data[station])<30):\n",
    "                weather_data[station] = trimmed_weather_df[trimmed_weather_df.Date.ge(prev_q+1) & trimmed_weather_df.Date.lt(date+1) & trimmed_weather_df.Station.eq(station)][['Date','Temperature']]\n",
    "            features.append(np.mean(weather_data[station].Temperature.values)/1e2)\n",
    "            feature_names.append(\"Temperature\")\n",
    "        dataset['weather']=weather_data\n",
    "\n",
    "        \n",
    "        dataset['features']=features\n",
    "        dataset['feature_names'] = feature_names\n",
    "        datasets[date] = dataset\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fit_model(datasets, holdout)\n",
    "Attempts to fit datasets, but ignoring out the last <holdout> dates for evaluation\n",
    "Returns a StanFit4Model object\n",
    "'''\n",
    "def fit_model(datasets, holdout):\n",
    "    train_dates = sorted(list (datasets.keys()))[:-holdout] if (holdout>0) else sorted(list (datasets.keys()))\n",
    "    nFeatures = len(datasets[train_dates[0]]['features'])\n",
    "    features = [datasets[date]['features'] for date in train_dates]\n",
    "    response_2 = [max(datasets[date]['ipc']['p2perc'], 1e-5) for date in train_dates]\n",
    "    response_3 = [max(datasets[date]['ipc']['p3perc'], 1e-5)  for date in train_dates]\n",
    "    response_4 = [max(datasets[date]['ipc']['p4perc'], 1e-5)  for date in train_dates]\n",
    "    famine_model_data = dict(\n",
    "        N = len(train_dates),\n",
    "        K = nFeatures,\n",
    "        feats = features,\n",
    "        response_2 = response_2,\n",
    "        response_3 = response_3,\n",
    "        response_4 = response_4\n",
    "    )\n",
    "    result = beta_model.sampling(data=famine_model_data, iter=3000, control = dict(max_treedepth=12, adapt_delta=0.8))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plot_model(datasets, model)\n",
    "Plots the predicted and actual IPC Phase proportion values based on the datasets and the StanFit4Model object\n",
    "'''\n",
    "\n",
    "def plot_model(datasets, model):\n",
    "    nFeatures = len(datasets[list(datasets.keys())[0]]['features'])\n",
    "    all_coeffs = list(map(lambda x: sum(x)/len(x), model.get_posterior_mean()))\n",
    "    al_2, be_2, co_2, k_2 = all_coeffs[0],all_coeffs[1:4],all_coeffs[4:4+nFeatures],all_coeffs[4+nFeatures]\n",
    "    al_3, be_3, co_3, k_3 = all_coeffs[5+nFeatures],all_coeffs[6+nFeatures:9+nFeatures],all_coeffs[9+nFeatures:9+2*nFeatures],all_coeffs[9+2*nFeatures]\n",
    "    al_4, be_4, co_4, k_4 = all_coeffs[10+2*nFeatures],all_coeffs[11+2*nFeatures:14+2*nFeatures],all_coeffs[14+2*nFeatures:14+3*nFeatures],all_coeffs[14+3*nFeatures]\n",
    "    \n",
    "    #Helper function to produce the predicted values\n",
    "    def generate(al_2, al_3, al_4, be_2, be_3, be_4, co_2, co_3, co_4, datasets):\n",
    "        times = sorted(list(datasets.keys()))\n",
    "        pred_ipc2_logit = [scipy.special.logit(max(datasets[times[0]]['ipc']['p2perc'], 1e-5))]\n",
    "        pred_ipc3_logit = [scipy.special.logit(max(datasets[times[0]]['ipc']['p3perc'], 1e-5))]\n",
    "        pred_ipc4_logit = [scipy.special.logit(max(datasets[times[0]]['ipc']['p4perc'], 1e-5))]\n",
    "        \n",
    "        for time in times[1:]:\n",
    "            new_ipc_2a = al_2 + be_2[0]*pred_ipc2_logit[-1] + be_2[1]*pred_ipc3_logit[-1] + be_2[2]*pred_ipc4_logit[-1] + sum(np.multiply(co_2, datasets[time]['features']))\n",
    "            new_ipc_3a = al_3 + be_3[0]*pred_ipc2_logit[-1] + be_3[1]*pred_ipc3_logit[-1] + be_3[2]*pred_ipc4_logit[-1] + sum(np.multiply(co_3, datasets[time]['features']))\n",
    "            new_ipc_4a = al_4 + be_4[0]*pred_ipc2_logit[-1] + be_4[1]*pred_ipc3_logit[-1] + be_4[2]*pred_ipc4_logit[-1] + sum(np.multiply(co_4, datasets[time]['features']))\n",
    "            pred_ipc2_logit.append(new_ipc_2a)\n",
    "            pred_ipc3_logit.append(new_ipc_3a)\n",
    "            pred_ipc4_logit.append(new_ipc_4a)\n",
    "        \n",
    "        pred_ipc2 = list(map(scipy.special.expit, pred_ipc2_logit))\n",
    "        pred_ipc3 = list(map(scipy.special.expit, pred_ipc3_logit))\n",
    "        pred_ipc4 = list(map(scipy.special.expit, pred_ipc4_logit))\n",
    "        return (pred_ipc2,pred_ipc3, pred_ipc4)\n",
    "    \n",
    "    (pred_ipc2, pred_ipc3, pred_ipc4) = generate(al_2, al_3, al_4, be_2, be_3, be_4, co_2, co_3, co_4, datasets)\n",
    "    print(\"IPC PHASE 2\")\n",
    "    times = sorted(list(datasets.keys()))\n",
    "    gold_ipc2 = [datasets[date]['ipc']['p2perc'] for date in times]\n",
    "    \n",
    "    plt.plot(times, gold_ipc2, marker='o', color='gold')\n",
    "    plt.plot(times,pred_ipc2, marker='+', color='red')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"IPC PHASE 3\")\n",
    "    gold_ipc3 = [datasets[date]['ipc']['p3perc'] for date in times]\n",
    "    \n",
    "    plt.plot(times, gold_ipc3, marker='o', color='gold')\n",
    "    plt.plot(times,pred_ipc3, marker='+', color='blue')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"IPC PHASE 4\")\n",
    "    gold_ipc4 = [datasets[date]['ipc']['p4perc'] for date in times]\n",
    "    \n",
    "    plt.plot(times, gold_ipc4, marker='o', color='gold')\n",
    "    plt.plot(times,pred_ipc4, marker='+', color='brown')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}